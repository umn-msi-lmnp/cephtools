


# ---------------------------------------------------------------------
# dd2dr
# ---------------------------------------------------------------------

describe "dd2dr" <<HEREDOC
---------------------------------------------------------------------
Usage:
    ${_ME} dd2dr [options] --group

Options:
    -g|--group <STRING>     MSI group ID (required)

    -t|--threads <INT>      Threads to use for uploading with rclone. [Default = 8].

    -l|--log_dir 	    Absolute or relative path to directory where log files are
    			    saved. [Default: "/home/GROUP/shared/dd2dr"]
			    
    -d|--dry_run            Dry run option will be applied to rclone commands. Nothing 
                            transfered or deleted when scripts run.
    
    -v|--verbose            Verbose mode (print additional info).

Description:
  Copy data from data delivery to disaster recovery. 
  
Help (print this screen):
    ${_ME} help dd2dr

Version: ${_VERSION}
Questions: Todd Knutson (knut0297@umn.edu) Christine O'Connor (oconnorc@umn.edu)
GitHub: https://github.umn.edu/lmnp/cephtools
---------------------------------------------------------------------
HEREDOC

dd2dr() {
#     echo ALL ARGS:
#     echo "${@:-}"
#     echo "${#}"


    
    # Parse Options ###############################################################

    # Initialize program option variables.
    local _group=
    local _dry_run=
    local _log_dir="/home/$(id -ng)/shared/dd2dr"
    local _threads=8
    local _verbose=0

    # __get_option_value()
    #
    # Usage:
    #   __get_option_value <option> <value>
    #
    # Description:
    #  Given a flag (e.g., -e | --example) return the value or exit 1 if value
    #  is blank or appears to be another option.
    __get_option_value() {
      local __arg="${1:-}"
      local __val="${2:-}"
      
      if [[ -n "${__val:-}" ]] && [[ ! "${__val:-}" =~ ^- ]]
      then
        printf "%s\\n" "${__val}"
      else
        _exit_1 printf "%s requires a valid argument.\\n" "${__arg}"
      fi
    }


    # For flags (i.e. no corresponding value), do not shift inside the case testing
    # statement. For options with required value, shift inside case testing statement, 
    # so the loop moves twice. 
    while ((${#}))
    do
        __arg="${1:-}"
        __val="${2:-}"

        case "${__arg}" in
        -d|--dry_run)
            _dry_run="--dry-run"
            ;;
        -v|--verbose)
            _verbose=1
            ;;
        -g|--group)
            _group="$(__get_option_value "${__arg}" "${__val:-}")"
            shift
            ;;
	-l|--log_dir)
	    _log_dir="$(__get_option_value "${__arg}" "${__val:-}")"
	    shift
	    ;;
        -t|--threads)
            _threads="$(__get_option_value "${__arg}" "${__val:-}")"
            shift
            ;;
        --endopts)
            # Terminate option parsing.
            break
            ;;
        -*)
            _exit_1 printf "Unexpected option: %s\\n" "${__arg}"
            ;;
        *)
            describe --get dd2dr
            _exit_1 printf "Unexpected positional arg: %s\\n" "${__arg}"
            ;;
        esac

        shift
    done


    # ---------------------------------------------------------------------
    # Check and print input options
    # ---------------------------------------------------------------------

    _verb printf -- "Program options used:\\n"
    _verb printf -- "--group: %s\\n" "$_group"
    _verb printf -- "--log_dir: %s\\n" "$_log_dir"
    _verb printf -- "--dry_run: %s\\n" "$_dry_run"
    _verb printf -- "--verbose: %s\\n" "$_verbose"
    _verb printf -- "--threads: %s\\n" "$_threads"


    # If required options are empty or null, exit.
    if [ -z "${_group}" ]
    then
        _exit_1 printf "The '--group' option must be specified and not be empty or null."
    fi

    # does log_dir need to be created?
    if [ ! -d "${_log_dir}" ]
    then
        _warn printf "The '--log_dir' option specified is not a valid directory. Creating the dir with g+rwx permissions: '%s'\\n" "${_log_dir}"
        mkdir -p ${_log_dir}
        chmod g+rwx ${_log_dir}
    fi

    # ---------------------------------------------------------------------
    # Check rclone specific info
    # ---------------------------------------------------------------------
    if command -v rclone &>/dev/null
    then
        _verb printf "Using rclone found in PATH:\\n"
        _verb printf "%s\\n" "$(command -v rclone)"
        _verb printf "%s\\n" "$(rclone --version)" 
    else
        _warn printf "rclone could not be found in PATH, so using the module: %s\\n" "/home/lmnp/knut0297/software/modulesfiles/rclone/1.57.0"
        MODULEPATH="/home/lmnp/knut0297/software/modulesfiles" module load rclone/1.57.0
        _verb printf "%s\\n" "$(command -v rclone)"
        _verb printf "%s\\n" "$(rclone --version)" 
    fi



    # ---------------------------------------------------------------------
    # Create working dir
    # ---------------------------------------------------------------------

    _archive_date_time="$(date +"%Y-%m-%d-%H%M%S")"
    _myprefix="dd2dr_${_archive_date_time}"    
    _myprefix_dir=${_log_dir}/${_myprefix}

    _verb printf "Archive dir name: %s\\n" ${_myprefix_dir}
    mkdir -p ${_myprefix_dir}
    chmod g+rwx ${_myprefix_dir}
    cd ${_myprefix_dir}
    

    # ---------------------------------------------------------------------
    # Check available space in group
    # ---------------------------------------------------------------------
    AVAIL=$( groupquota -g ${_group} -p -U '' -cH | awk 'BEGIN { FS=",";OFS=","} {print $3-$2}' )
    AVAILH=$( groupquota -g ${_group} -p -U 'G' -cH | sed 's/G//g' | awk 'BEGIN { FS=",";OFS=","} {print $3-$2}' )
    DDTOTAL=$(du -Lhc /home/${_group}/data_delivery | tail -1 | cut -f1)
    DDBYTES=$(du -Lbc /home/${_group}/data_delivery |tail -1 | cut -f1)
    DDBYTES_TRANSFER=$(rsync -Lvru --dry-run --stats /home/${_group}/data_delivery /home/${_group}/shared/disaster_recovery/ | grep "Total transferred file size:" | tr " " "\t" | cut -f 5 | sed 's/\,//g')
    DDDRBYTES=$(du -Lbc /home/${_group}/data_delivery |tail -1 | cut -f1)

    # ---------------------------------------------------------------------
    # Determine best slurm partition
    # ---------------------------------------------------------------------

    # Capture the dns domain name for current cluster
    cluster=$(dnsdomainname | awk -F. '{print $1}')

    # Specify the appropriate slurm partition in job files
    if [[ "${cluster}" = "agate" ]]; then
        _partition="msismall"
    elif [[ "${cluster}" = "mesabi" ]]; then
        _partition="msismall"
    elif [[ "${cluster}" = "mangi" ]]; then
        _partition="msismall"
    else
        _partition="msismall"
    fi


    #######################################################################
    # Copy
    #######################################################################


    tee ${_myprefix}.1_dd2dr.slurm << HEREDOC > /dev/null
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=${_threads}
#SBATCH --time=24:00:00
#SBATCH --mem=32gb
#SBATCH --error=%x.e%j
#SBATCH --output=%x.o%j
#SBATCH --mail-type=BEGIN,END,FAIL
#SBATCH --mail-user=$USER@umn.edu
#SBATCH --partition=$_partition

echo "[dd2dr "\$(date)"] Script start."


# ---------------------------------------------------------------------
# Variables
# ---------------------------------------------------------------------


_myprefix=${_myprefix}
_myprefix_dir=${_myprefix_dir}
_dry_run=${_dry_run}
_group=${_group}
_threads=${_threads}
_archive_date_time=${_archive_date_time}


# ---------------------------------------------------------------------
# Load software
# ---------------------------------------------------------------------

cd \${_myprefix_dir}

MODULEPATH="/home/lmnp/knut0297/software/modulesfiles" module load rclone/1.57.0



# ---------------------------------------------------------------------
# Print some info
# ---------------------------------------------------------------------

echo "${_group} sync starting..."
#DATESTAMP=$(date +%Y%m%d%M%S)
echo "${_archive_date_time}"
#exit 6
umask u=rwx,g=rx,o=



# ---------------------------------------------------------------------
# Check for any space issues
# ---------------------------------------------------------------------

# should I rely on groupquota or are there other lower level system tools that I could use...
groupquota -g ${_group} -p -U 'G' -c
AVAIL=$( groupquota -g ${_group} -p -U '' -cH | awk 'BEGIN { FS=",";OFS=","} {print $3-$2}' )
AVAILH=$( groupquota -g ${_group} -p -U 'G' -cH | sed 's/G//g' | awk 'BEGIN { FS=",";OFS=","} {print $3-$2}' )

echo "${AVAILH}G remaining in /home/${_group} total quota"


# ---------------------------------------------------------------------
# Check data_delivery
# ---------------------------------------------------------------------

#DDTOTAL=$(du -Lhc /home/$GROUP/oconnorc/data_delivery_backup/data_delivery | tail -1 | cut -f1)
DDTOTAL=$(du -Lhc /home/${_group}/data_delivery | tail -1 | cut -f1)
echo "$DDTOTAL in /home/${_group}/data_delivery"
# must use -b to get bytes that are comparable to $AVAIL from groupquota
#DDBYTES=$(du -Lbc /home/$GROUP/oconnorc/data_delivery_backup/data_delivery |tail -1 | cut -f1)
DDBYTES=$(du -Lbc /home/${_group}/data_delivery |tail -1 | cut -f1)
echo "$DDBYTES bytes in data_delivery"


# ---------------------------------------------------------------------
# Total size of files that will be transfered
# ---------------------------------------------------------------------

#DDBYTES_TRANSFER=$(rsync -Lvru --dry-run --stats /home/$GROUP/oconnorc/data_delivery_backup/data_delivery /home/$GROUP/oconnorc/data_delivery_backup/disaster_recovery/ | grep "Total transferred file size:" | tr " " "\t" | cut -f 5 | sed 's/\,//g')
DDBYTES_TRANSFER=$(rsync -Lvru --dry-run --stats /home/${_group}/data_delivery /home/${_group}/shared/disaster_recovery/ | grep "Total transferred file size:" | tr " " "\t" | cut -f 5 | sed 's/\,//g')
echo "$DDBYTES_TRANSFER bytes to be transferred"


# ---------------------------------------------------------------------
# Transfer the files
# ---------------------------------------------------------------------

#if [ "$DDBYTES" -lt "$AVAIL" ]; then
if [ "$DDBYTES_TRANSFER" -lt "$AVAIL" ]; then
    echo "$DDBYTES_TRANSFER < $AVAIL, syncing data_delivery to disaster_recovery"
    #echo "$DDBYTES < $AVAIL, syncing data_delivery to disaster_recovery"
    # -L is critical to copy links as files...
    rsync -Lvru /home/${_group}/data_delivery /home/${_group}/shared/disaster_recovery/
# add a check here to make sure the rsync finished successfully.
    if [ "$?" -eq 0 ]; then
       echo "rsync finished successfully!"
    else
       echo "rsync did not finish successfully"
       exit 5
    fi
    
    AVAILHNEW=$( groupquota -g ${_group} -U 'G' -cH | sed 's/G//g' | awk 'BEGIN { FS=",";OFS=","} {print $3-$2}' )
    echo "Previous space available was ${AVAILH}"
    echo "New space available is ${AVAILHNEW}"
    echo "Sync complete"
else
    echo "$DDBYTES > $AVAIL Not enough space for syncing!"
    echo "Checking disaster_recovery data_delivery size..."
    DDDRBYTES=$(du -Lbc /home/${_group}/data_delivery |tail -1 | cut -f1)
    echo "$DDDRBYTES in disaster_recovery data_delivery"
    if [ "$DDBYTES" -eq "$DDDRBYTES" ]; then
	echo "$DDBYTES (dd size) equal to $DDDRBYTES (dddr size)"
	echo "Nothing needs to transfer, but space is not available"
    else
# need to update here to deal with the scenario where dd data has rolled off the system
	# dd could be smaller than dddr but data needs to sync
	# can dd be larger than dddr - possibly, but under what conditions?
    # maybe the best thing to do is check to see if the most recent data transferred successfully
      NEW=$(du -Lbc /home/${_group}/shared/disaster_recovery/data_delivery |tail -4 | head -1)
      QUARTER=$(echo $NEW | cut -d '/' -f 8)
      NEWSIZE=$(du -Lbc /home/${_group}/shared/disaster_recovery/data_delivery |tail -4 | head -1 | cut -f1)
      MATCHSIZE=$(du -Lbc /home/${_group}/data_delivery/*/$QUARTER | tail -1 | cut -f1 ) 
     if [ "$NEWSIZE" -eq "$MATCHSIZE" ]; then
	 echo "Most recent data has been synced, warning that space is not available to sync, exit code 10"
	 exit 10
     else
	echo "Newest data has not been synced and space is not available, exiting with code 20"
	exit 20
     fi 
    fi

fi


# ---------------------------------------------------------------------
# Job summary info
# ---------------------------------------------------------------------

echo "[dd2dr "\$(date)"] Script end."

if [ ! -z \${SLURM_JOB_ID+x} ]; then
    scontrol show job "\${SLURM_JOB_ID}"
    sstat -j "\${SLURM_JOB_ID}" --format=JobID,MaxRSS,MaxVMSize,NTasks,MaxDiskWrite,MaxDiskRead
fi



HEREDOC



# ---------------------------------------------------------------------
# Launch jobs
# ---------------------------------------------------------------------



if ((_dry_run))
then
    # Do no launch the slurm jobs
    _warn printf "Launch option --dry-run was specified. Dirs and slurm job files created, but not launched.\\n"
else
        sbatch_output=$(sbatch "${_myprefix}.1_dd2dr.slurm")
        job_id=$(echo "${sbatch_output}" | awk '{print $4}')
        sbatch_job_ids+=("${job_id}")
        _verb printf "%s\n" "${sbatch_output}"
        _info printf "$${_myprefix}.1_dd2dr.slurm job file created and launched.\n"
fi



    #######################################################################
    # Readme
    #######################################################################



#    tee ${_myprefix}.readme.md << HEREDOC > /dev/null
## dd2dr archive

#Archive initated (Y-m-d-HMS):  
#${_archive_date_time}   

### Introduction

#The PI's data_delivery directory was backed up to their disaster_recovery space. All data inside the directory was copied using \`rclone\` which uses MD5 checksums on every transfer to ensure data integrity. This is necessary as data only stays in data_delivery for a year and often PIs need access to data for more than a year before publication.  


### How this works

#* A program called \`cephtools dd2dr\` found all the files in the directory above.
#* Then the program created a slurm job file (bash script) that can be run to copy all files from data_delivery to disaster_recovery.
#* The program also created this README file.
#* After running the slurm job scripts, you should review the stderr and stdout log files and the new file lists that get created during the \`*.1_copy.slurm\` job.


### Notes:

#* A new dir is created next to the original that contains these archive-related files and scripts. This dir should have the same permissions as the original dir (that got archived). 
#* File modification times/dates should be preserved between transfers.



### Variables


#Variable Name | Value 
#-----------|----------
#Archive date and time | ${_archive_date_time}
#Archive transfer prefix | ${_myprefix}
#Directory path for archive files | ${_myprefix_dir}
#User doing transfer | $USER



#HEREDOC




}