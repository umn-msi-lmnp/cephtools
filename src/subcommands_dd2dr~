


# ---------------------------------------------------------------------
# dd2dr
# ---------------------------------------------------------------------

describe "dd2dr" <<HEREDOC
---------------------------------------------------------------------
Usage:
    ${_ME} dd2dr [options] --group

Options:
    -g|--group <STRING>     MSI group ID (required)

    -t|--threads <INT>      Threads to use for uploading with rclone. [Default = 8].

    -l|--log_dir 	    Absolute or relative path to directory where log files are
    			    saved. [Default: "/home/GROUP/shared/dd2dr"]
			    
    -d|--dry_run            Dry run option will be applied to rclone commands. Nothing 
                            transfered or deleted when scripts run.
    
    -v|--verbose            Verbose mode (print additional info).

Description:
  Copy data from data delivery to disaster recovery. 
  
Help (print this screen):
    ${_ME} help dd2dr

Version: ${_VERSION}
Questions: Todd Knutson (knut0297@umn.edu) Christine O'Connor (oconnorc@umn.edu)
GitHub: https://github.umn.edu/lmnp/cephtools
---------------------------------------------------------------------
HEREDOC

dd2dr() {
#     echo ALL ARGS:
#     echo "${@:-}"
#     echo "${#}"


    
    # Parse Options ###############################################################

    # Initialize program option variables.
    local _group=
    local _dry_run=
    local _log_dir="/home/$(id -ng)/shared/dd2dr"
    local _threds=8
    local _verbose=0

    # __get_option_value()
    #
    # Usage:
    #   __get_option_value <option> <value>
    #
    # Description:
    #  Given a flag (e.g., -e | --example) return the value or exit 1 if value
    #  is blank or appears to be another option.
    __get_option_value() {
      local __arg="${1:-}"
      local __val="${2:-}"
      
      if [[ -n "${__val:-}" ]] && [[ ! "${__val:-}" =~ ^- ]]
      then
        printf "%s\\n" "${__val}"
      else
        _exit_1 printf "%s requires a valid argument.\\n" "${__arg}"
      fi
    }


    # For flags (i.e. no corresponding value), do not shift inside the case testing
    # statement. For options with required value, shift inside case testing statement, 
    # so the loop moves twice. 
    while ((${#}))
    do
        __arg="${1:-}"
        __val="${2:-}"

        case "${__arg}" in
        -d|--dry_run)
            _dry_run="--dry-run"
            ;;
        -v|--verbose)
            _verbose=1
            ;;
        -g|--group)
            _group="$(__get_option_value "${__arg}" "${__val:-}")"
            shift
            ;;
	-l|--log_dir
	    _log_dir="$(__get_option_value "${__arg}" "${__val:-}")"
	    shift
	    ;;
        -t|--threads)
            _threads="$(__get_option_value "${__arg}" "${__val:-}")"
            shift
            ;;
        --endopts)
            # Terminate option parsing.
            break
            ;;
        -*)
            _exit_1 printf "Unexpected option: %s\\n" "${__arg}"
            ;;
        *)
            describe --get dd2dr
            _exit_1 printf "Unexpected positional arg: %s\\n" "${__arg}"
            ;;
        esac

        shift
    done


    # ---------------------------------------------------------------------
    # Check and print input options
    # ---------------------------------------------------------------------

    _verb printf -- "Program options used:\\n"
    _verb printf -- "--group: %s\\n" "$_group"
    _verb printf -- "--log_dir: %s\\n" "$_log_dir"
    _verb printf -- "--dry_run: %s\\n" "$_dry_run"
    _verb printf -- "--verbose: %s\\n" "$_verbose"
    _verb printf -- "--delete_empty_dirs: %s\\n" "$_delete_empty_dirs"
    _verb printf -- "--threads: %s\\n" "$_threads"


    # If required options are empty or null, exit.
    if [ -z "${_group}" ]
    then
        _exit_1 printf "The '--group' option must be specified and not be empty or null."
    fi

    # does log_dir need to be created?
    if [ ! -d "${_log_dir}" ]
    then
        _warn printf "The '--log_dir' option specified is not a valid directory. Creating the dir with g+rwx permissions: '%s'\\n" "${_log_dir}"
        mkdir -p ${_log_dir}
        chmod g+rwx ${_log_dir}
    fi

    # ---------------------------------------------------------------------
    # Check rclone specific info
    # ---------------------------------------------------------------------
    if command -v rclone &>/dev/null
    then
        _verb printf "Using rclone found in PATH:\\n"
        _verb printf "%s\\n" "$(command -v rclone)"
        _verb printf "%s\\n" "$(rclone --version)" 
    else
        _warn printf "rclone could not be found in PATH, so using the module: %s\\n" "/home/lmnp/knut0297/software/modulesfiles/rclone/1.57.0"
        MODULEPATH="/home/lmnp/knut0297/software/modulesfiles" module load rclone/1.57.0
        _verb printf "%s\\n" "$(command -v rclone)"
        _verb printf "%s\\n" "$(rclone --version)" 
    fi



    # ---------------------------------------------------------------------
    # Create archive working dir
    # ---------------------------------------------------------------------

    _archive_date_time="$(date +"%Y-%m-%d-%H%M%S")"
    


    # ---------------------------------------------------------------------
    # Determine best slurm partition
    # ---------------------------------------------------------------------

    # Capture the dns domain name for current cluster
    cluster=$(dnsdomainname | awk -F. '{print $1}')

    # Specify the appropriate slurm partition in job files
    if [[ "${cluster}" = "agate" ]]; then
        _partition="msismall"
    elif [[ "${cluster}" = "mesabi" ]]; then
        _partition="msismall"
    elif [[ "${cluster}" = "mangi" ]]; then
        _partition="msismall"
    else
        _partition="msismall"
    fi


    #######################################################################
    # Copy
    #######################################################################


    tee ${_myprefix}.1_dd2dr.slurm << HEREDOC > /dev/null
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=${_threads}
#SBATCH --time=24:00:00
#SBATCH --mem=32gb
#SBATCH --error=%x.e%j
#SBATCH --output=%x.o%j
#SBATCH --mail-type=BEGIN,END,FAIL
#SBATCH --mail-user=$USER@umn.edu
#SBATCH --partition=$_partition

echo "[dd2dr "\$(date)"] Script start."


# ---------------------------------------------------------------------
# Variables
# ---------------------------------------------------------------------


_root_path_dir=$_root_path_dir
_myprefix=${_myprefix}
_myprefix_dir=${_myprefix_dir}
_dry_run=${_dry_run}
_group=${_group}
_threads=${_threads}



# ---------------------------------------------------------------------
# Load software
# ---------------------------------------------------------------------

cd \${_myprefix_dir}

MODULEPATH="/home/lmnp/knut0297/software/modulesfiles" module load rclone/1.57.0



# ---------------------------------------------------------------------
# Print some info
# ---------------------------------------------------------------------

echo "${_group} sync starting..."
DATESTAMP=$(date +%Y%m%d%M%S)
echo "${DATESTAMP}"
#exit 6
umask u=rwx,g=rx,o=



# ---------------------------------------------------------------------
# Check for any space issues
# ---------------------------------------------------------------------

# should I rely on groupquota or are there other lower level system tools that I could use...
groupquota -g ${_group} -p -U 'G' -c
AVAIL=$( groupquota -g ${_group} -p -U '' -cH | awk 'BEGIN { FS=",";OFS=","} {print $3-$2}' )
AVAILH=$( groupquota -g ${_group} -p -U 'G' -cH | sed 's/G//g' | awk 'BEGIN { FS=",";OFS=","} {print $3-$2}' )

echo "${AVAILH}G remaining in /home/${_group} total quota"


# ---------------------------------------------------------------------
# Check data_delivery
# ---------------------------------------------------------------------

#DDTOTAL=$(du -Lhc /home/$GROUP/oconnorc/data_delivery_backup/data_delivery | tail -1 | cut -f1)
DDTOTAL=$(du -Lhc /home/${_group}/data_delivery | tail -1 | cut -f1)
echo "$DDTOTAL in /home/${_group}/data_delivery"
# must use -b to get bytes that are comparable to $AVAIL from groupquota
#DDBYTES=$(du -Lbc /home/$GROUP/oconnorc/data_delivery_backup/data_delivery |tail -1 | cut -f1)
DDBYTES=$(du -Lbc /home/${_group}/data_delivery |tail -1 | cut -f1)
echo "$DDBYTES bytes in data_delivery"


# ---------------------------------------------------------------------
# Total size of files that will be transfered
# ---------------------------------------------------------------------

#DDBYTES_TRANSFER=$(rsync -Lvru --dry-run --stats /home/$GROUP/oconnorc/data_delivery_backup/data_delivery /home/$GROUP/oconnorc/data_delivery_backup/disaster_recovery/ | grep "Total transferred file size:" | tr " " "\t" | cut -f 5 | sed 's/\,//g')
DDBYTES_TRANSFER=$(rsync -Lvru --dry-run --stats /home/${_group}/data_delivery /home/${_group}/shared/disaster_recovery/ | grep "Total transferred file size:" | tr " " "\t" | cut -f 5 | sed 's/\,//g')
echo "$DDBYTES_TRANSFER bytes to be transferred"


# ---------------------------------------------------------------------
# Transfer the files
# ---------------------------------------------------------------------

#if [ "$DDBYTES" -lt "$AVAIL" ]; then
if [ "$DDBYTES_TRANSFER" -lt "$AVAIL" ]; then
    echo "$DDBYTES_TRANSFER < $AVAIL, syncing data_delivery to disaster_recovery"
    #echo "$DDBYTES < $AVAIL, syncing data_delivery to disaster_recovery"
    # -L is critical to copy links as files...
    rsync -Lvru /home/${_group}/data_delivery /home/${_group}/shared/disaster_recovery/
# add a check here to make sure the rsync finished successfully.
    if [ "$?" -eq 0 ]; then
       echo "rsync finished successfully!"
    else
       echo "rsync did not finish successfully"
       exit 5
    fi
    
    AVAILHNEW=$( groupquota -g ${_group} -U 'G' -cH | sed 's/G//g' | awk 'BEGIN { FS=",";OFS=","} {print $3-$2}' )
    echo "Previous space available was ${AVAILH}"
    echo "New space available is ${AVAILHNEW}"
    echo "Sync complete"
else
    echo "$DDBYTES > $AVAIL Not enough space for syncing!"
    echo "Checking disaster_recovery data_delivery size..."
    DDDRBYTES=$(du -Lbc /home/${_group}/data_delivery |tail -1 | cut -f1)
    echo "$DDDRBYTES in disaster_recovery data_delivery"
    if [ "$DDBYTES" -eq "$DDDRBYTES" ]; then
	echo "$DDBYTES (dd size) equal to $DDDRBYTES (dddr size)"
	echo "Nothing needs to transfer, but space is not available"
    else
# need to update here to deal with the scenario where dd data has rolled off the system
	# dd could be smaller than dddr but data needs to sync
	# can dd be larger than dddr - possibly, but under what conditions?
    # maybe the best thing to do is check to see if the most recent data transferred successfully
      NEW=$(du -Lbc /home/${_group}/shared/disaster_recovery/data_delivery |tail -4 | head -1)
      QUARTER=$(echo $NEW | cut -d '/' -f 8)
      NEWSIZE=$(du -Lbc /home/${_group}/shared/disaster_recovery/data_delivery |tail -4 | head -1 | cut -f1)
      MATCHSIZE=$(du -Lbc /home/${_group}/data_delivery/*/$QUARTER | tail -1 | cut -f1 ) 
     if [ "$NEWSIZE" -eq "$MATCHSIZE" ]; then
	 echo "Most recent data has been synced, warning that space is not available to sync, exit code 10"
	 exit 10
     else
	echo "Newest data has not been synced and space is not available, exiting with code 20"
	exit 20
     fi 
    fi

fi


# ---------------------------------------------------------------------
# Job summary info
# ---------------------------------------------------------------------

echo "[panfs2ceph "\$(date)"] Script end."

if [ ! -z \${SLURM_JOB_ID+x} ]; then
    scontrol show job "\${SLURM_JOB_ID}"
    sstat -j "\${SLURM_JOB_ID}" --format=JobID,MaxRSS,MaxVMSize,NTasks,MaxDiskWrite,MaxDiskRead
fi



HEREDOC







    #######################################################################
    # Delete
    #######################################################################


    tee ${_myprefix}.2_delete.slurm << HEREDOC > /dev/null
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --time=24:00:00
#SBATCH --mem=32gb
#SBATCH --error=%x.e%j
#SBATCH --output=%x.o%j
#SBATCH --mail-type=BEGIN,END,FAIL
#SBATCH --mail-user=$USER@umn.edu
#SBATCH --partition=$_partition

echo "[panfs2ceph "\$(date)"] Script start."


# ---------------------------------------------------------------------
# Variables
# ---------------------------------------------------------------------


_root_path_dir=${_root_path_dir}
_myprefix=${_myprefix}
_myprefix_dir=${_myprefix_dir}
_dry_run=${_dry_run}




# ---------------------------------------------------------------------
# Software
# ---------------------------------------------------------------------

cd \${_myprefix_dir}

MODULEPATH="/home/lmnp/knut0297/software/modulesfiles" module load rsync/3.1.2

# ---------------------------------------------------------------------
# Delete files from panfs
# ---------------------------------------------------------------------

# Delete all files in the file list
# Use rsync to delte files because it's faster than "rm"

_empty_dir=\${_myprefix_dir}/empty_dir

mkdir -p \${_empty_dir}

rsync \${_dry_run} -v -r --force --delete \${_empty_dir}/ \${_root_path_dir}/



if [ ! \$? -eq 0 ]
then
    # rclone exit status was not zero
    echo "[panfs2ceph "\$(date)"] Delete process: FAIL" 2>&1 | tee \${_myprefix}.DELETE_FAILED
else
    echo "[panfs2ceph "\$(date)"] Delete process: SUCCESS"
fi

# Delete the original archive dir and the temp empty dir
rm -rf \${_root_path_dir}
rm -rf \${_empty_dir}





# ---------------------------------------------------------------------
# Job summary info
# ---------------------------------------------------------------------

echo "[panfs2ceph "\$(date)"] Script end."

if [ ! -z \${SLURM_JOB_ID+x} ]; then
    scontrol show job "\${SLURM_JOB_ID}"
    sstat -j "\${SLURM_JOB_ID}" --format=JobID,MaxRSS,MaxVMSize,NTasks,MaxDiskWrite,MaxDiskRead
fi



HEREDOC







    #######################################################################
    # Restore
    #######################################################################


    tee ${_myprefix}.3_restore.slurm << HEREDOC > /dev/null
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=${_threads}
#SBATCH --time=24:00:00
#SBATCH --mem=32gb
#SBATCH --error=%x.e%j
#SBATCH --output=%x.o%j
#SBATCH --mail-type=BEGIN,END,FAIL
#SBATCH --mail-user=$USER@umn.edu
#SBATCH --partition=$_partition

echo "[panfs2ceph "\$(date)"] Script start."


# ---------------------------------------------------------------------
# Variables
# ---------------------------------------------------------------------


_root_path_dir=${_root_path_dir}
_myprefix=${_myprefix}
_myprefix_dir=${_myprefix_dir}
_dry_run=${_dry_run}
_remote=${_remote}
_bucket=${_bucket}
_threads=${_threads}



# ---------------------------------------------------------------------
# Load software
# ---------------------------------------------------------------------



MODULEPATH="/home/lmnp/knut0297/software/modulesfiles" module load rclone/1.57.0



# ---------------------------------------------------------------------
# Check if original dir still exists on panfs
# ---------------------------------------------------------------------

if [ -d "\${_root_path_dir}" ]
then
    echo "[panfs2ceph "\$(date)"] The original dir (\${_root_path_dir}) still exists on panfs. Stopping restore process. Change the dir name on panfs and re-run restore script." >&2
    exit 1
fi



# ---------------------------------------------------------------------
# rclone
# ---------------------------------------------------------------------

# Nudge rclone to list the files on bucket (refresh). This seems to help avoid random errors.
rclone lsf -R --links \${_remote}:\${_bucket} --max-depth 1 > /dev/null


# Copy the files from ceph back to panfs
rclone copy "\${_remote}:\${_bucket}\${_root_path_dir}" "\${_root_path_dir}" --links \${_dry_run} -v --transfers \${_threads} --s3-chunk-size 50M --s3-upload-concurrency 10


if [ ! \$? -eq 0 ]
then
    # rclone exit status was not zero
    echo "[panfs2ceph "\$(date)"] rclone restore process: FAIL" 2>&1 | tee \${_myprefix}.RESTORE_FAILED
else
    echo "[panfs2ceph "\$(date)"] rclone restore process: SUCCESS"
fi



# ---------------------------------------------------------------------
# Job summary info
# ---------------------------------------------------------------------

echo "[panfs2ceph "\$(date)"] Script end."

if [ ! -z \${SLURM_JOB_ID+x} ]; then
    scontrol show job "\${SLURM_JOB_ID}"
    sstat -j "\${SLURM_JOB_ID}" --format=JobID,MaxRSS,MaxVMSize,NTasks,MaxDiskWrite,MaxDiskRead
fi







HEREDOC






    #######################################################################
    # Readme
    #######################################################################



    tee ${_myprefix}.readme.md << HEREDOC > /dev/null
# panfs2ceph archive

Archive initated (Y-m-d-HMS):  
${_archive_date_time}   

## Introduction

This directory (\`${_root_path_dir}\`) was archived from MSI's *panfs* (tier 1) storage to its *ceph* tier 2 storge. All data inside the directory was copied using \`rclone\` which uses MD5 checksums on every transfer to ensure data integrity. After the data was copied, it was deleted from \`panfs\`. 


## How this works

* A program called \`cephtools panfs2ceph\` found all the files in the directory above and created a file list text file.
* Then the program created a slurm job file (bash script) that can be run to copy all files from panfs to ceph.
* The program also created a slurm job file (bash script) to delete the original data from panfs.
* The program also created this README file.
* The program does not actually run these slrum job scripts. You must do that manually after reviewing the file list, etc.
* After running the slurm job scripts, you should review the stderr and stdout log files and the new file lists that get created during the \`*.1_copy.slurm\` job.


## Notes:

* A new dir is created next to the original that contains these archive-related files and scripts. This dir should have the same permissions as the original dir (that got archived). 
* File modification times/dates should be preserved between transfers. However, if files are transferred from ceph back to panfs, the original modification times of directories will be lost (i.e. block storage like tier 2 does not have directories, so their mod times can't be preserved).



## Variables


Variable Name | Value 
-----------|----------
Archive date and time | ${_archive_date_time}
Directory to archive | ${_root_path_dir}
Archive transfer prefix | ${_myprefix}
Directory path for archive files | ${_myprefix_dir}
Ceph UserID | $(s3info info | grep username | awk -F': ' '{print $2}')
Ceph Remote Name | ${_remote}
Ceph Bucket Name | ${_bucket}
User doing transfer | $USER


## How to access the original data

### Browse the data on ceph

You can use a variety of tools to browse data stored on ceph. For example:

1. rclone (e.g. \`rclone ls ${_remote}:${_bucket}${_root_path_dir}\`). This requires a properly configured \`~/.config/rclone/rclone.conf\` file. 
2. s3cmd (e.g. \`s3cmd ls -r s3://${_bucket}${_root_path_dir}\`). This requires a properly configured \`~/.s3cfg\` file. Note: different MSI users might use different rclone remote names -- use *your* remote name. 
3. GUI based file browser (e.g. FileZilla, Panic's Transmit, CyberDuck, etc.)


### Browse the data via panfs \`snapshot\`

The original data was deleted, but might still be stored on \`panfs\` as a snapshot for a short period of time (approximately 1 month or less). If this data was archived to ceph and deleted from panfs, it might be recovered from panfs for a short time. Review the available snapshots here:

    ${_root_path_dir}/.snapshot


### Restore from ceph back to panfs

Depending on user permissions, the data can be copied from ceph back to panfs using \`rclone\` or other methods. For example, running \`${_myprefix}.3_restore.slurm\` job script should copy the data from ceph back to panfs in its original location.



HEREDOC




    #######################################################################
    # Print instructions to terminal
    #######################################################################


    #read -r -d '' instructions_message << HEREDOC > /dev/null
    # The unix "read" command does not return a zero exit code. Use a temp function instead.
    # https://stackoverflow.com/a/8088167/2367748
    heredoc2var(){ IFS='\n' read -r -d '' ${1} || true; }
    
    heredoc2var instructions_message << HEREDOC > /dev/null

---------------------------------------------------------------------
cephtools panfs2ceph summary

Options used:
dry_run=${_dry_run}
delete_empty_dirs=${_delete_empty_dirs}
remote=${_remote}
bucket=${_bucket}
threads=${_threads}

Archive dir: 
${_root_path_dir}

Archive dir transfer scripts:
${_myprefix_dir}

Achrive transfer files created -- but you're not done yet!
Next steps:
1. Move into transfer dir: cd ${_myprefix_dir}
2. Review the *.readme.md file for details.
3. Review the *.filelist.txt file. The files in this list will be copied to ceph and delted from panfs.
4. Launch the copy jobfile: sbatch ${_myprefix}.1_copy.slurm
5. After sucessful copy, launch the delete jobfile: sbatch ${_myprefix}.2_delete.slurm
6. After the data has been deleted from panfs -- and you need it back in the same location, launch the restore jobfile: sbatch ${_myprefix}.3_restore.slurm


VERSION: ${_VERSION}
QUESTIONS: Todd Knutson (knut0297@umn.edu)
REPO: https://github.umn.edu/knut0297org/cephtools
---------------------------------------------------------------------
HEREDOC


    echo "$instructions_message"


}



