


# ---------------------------------------------------------------------
# panfs2ceph
# ---------------------------------------------------------------------

describe "panfs2ceph" <<HEREDOC
---------------------------------------------------------------------
Usage:
    ${_ME} panfs2ceph [options] --bucket <BUCKET> --remote <REMOTE> --path <DIR_PATH>

Options:
    -r|--remote <STRING>    Rclone remote name. (use "rclone listremotes" for available
                            remotes). Rclone remotes must be set up using "rclone init"
                            and can be viewed at: ~/.config/rclone/rclone.conf.
                            
    -b|--bucket <STRING>    Name of the ceph bucket that data should be used for the 
                            transfer.
    
    -p|--path <STRING>      Absolute or relative path to the directory that should be 
                            transfered.
                            
    -l|--log_dir <STRING>   Absolute or relative path to the directory where log files 
                            are saved. [Default: "<path>___panfs2ceph_archive_<date_time>"]
    
    -d|--dry_run            Dry run option will be applied to rclone commands. Nothing 
                            transfered or deleted when scripts run.
    
    -e|--delete_empty_dirs  Do NOT transfer empty dirs on panfs to ceph. [Default is to 
                            create a hidden file (".empty_dir") inside all empty dirs so 
                            they get transfered to ceph. Setting this flag will not create
                            the ".empty_dir" files, thus empty dirs will not get copied to
                            ceph.
                            
    -t|--threads <INT>      Threads to use for uploading with rclone. [Default = 16].
    
    -v|--verbose            Verbose mode (print additional info).

Description:
  Archiving tool to copy a single directory from tier 1 (panfs) storage to tier 2 (ceph).
  
Help (print this screen):
    ${_ME} help panfs2ceph

Version: ${VERSION_SHORT}
Questions: Todd Knutson (knut0297@umn.edu) or Christine O'Connor (oconnorc@umn.edu)
GitHub: https://github.umn.edu/lmnp/cephtools
---------------------------------------------------------------------
HEREDOC

panfs2ceph() {
#     echo ALL ARGS:
#     echo "${@:-}"
#     echo "${#}"


    
    # Parse Options ###############################################################

    # Initialize program option variables.
    local _bucket=
    local _remote=
    local _path=
    local _log_dir=
    local _dry_run=
    local _verbose=0
    local _delete_empty_dirs=0
    local _threads="16"

    # __get_option_value()
    #
    # Usage:
    #   __get_option_value <option> <value>
    #
    # Description:
    #  Given a flag (e.g., -e | --example) return the value or exit 1 if value
    #  is blank or appears to be another option.
    __get_option_value() {
      local __arg="${1:-}"
      local __val="${2:-}"
      
      if [[ -n "${__val:-}" ]] && [[ ! "${__val:-}" =~ ^- ]]
      then
        printf "%s\\n" "${__val}"
      else
        _exit_1 printf "%s requires a valid argument.\\n" "${__arg}"
      fi
    }


    # For flags (i.e. no corresponding value), do not shift inside the case testing
    # statement. For options with required value, shift inside case testing statement, 
    # so the loop moves twice. 
    while ((${#}))
    do
        __arg="${1:-}"
        __val="${2:-}"

        case "${__arg}" in
        -d|--dry_run)
            _dry_run="--dry-run"
            ;;
        -v|--verbose)
            _verbose=1
            ;;
        -e|--delete_empty_dirs)
            _delete_empty_dirs=1
            ;;
        -b|--bucket)
            _bucket="$(__get_option_value "${__arg}" "${__val:-}")"
            shift
            ;;
        -r|--remote)
            _remote="$(__get_option_value "${__arg}" "${__val:-}")"
            shift
            ;;
        -p|--path)
            _path="$(__get_option_value "${__arg}" "${__val:-}")"
            shift
            ;;
        -l|--log_dir)
            _log_dir="$(__get_option_value "${__arg}" "${__val:-}")"
            shift
            ;;
        -t|--threads)
            _threads="$(__get_option_value "${__arg}" "${__val:-}")"
            shift
            ;;
        --endopts)
            # Terminate option parsing.
            break
            ;;
        -*)
            _exit_1 printf "Unexpected option: %s\\n" "${__arg}"
            ;;
        *)
            describe --get panfs2ceph
            _exit_1 printf "Unexpected positional arg: %s\\n" "${__arg}"
            ;;
        esac

        shift
    done


    # ---------------------------------------------------------------------
    # Check and print input options
    # ---------------------------------------------------------------------

    _verb printf -- "Program options used:\\n"
    _verb printf -- "--bucket: %s\\n" "$_bucket"
    _verb printf -- "--remote: %s\\n" "$_remote"
    _verb printf -- "--path: %s\\n" "$_path"
    _verb printf -- "--log_dir: %s\\n" "$_log_dir"
    _verb printf -- "--dry_run: %s\\n" "$_dry_run"
    _verb printf -- "--verbose: %s\\n" "$_verbose"
    _verb printf -- "--delete_empty_dirs: %s\\n" "$_delete_empty_dirs"
    _verb printf -- "--threads: %s\\n" "$_threads"


    # If required options are empty or null, exit.
    if [ -z "${_remote}" ]
    then
        _exit_1 printf "The '--remote' option must be specified and not be empty or null."
    fi
    if [ -z "${_bucket}" ]
    then
        _exit_1 printf "The '--bucket' option must be specified and not be empty or null."
    fi
    if [ -z "${_path}" ]
    then
        _exit_1 printf "The '--path' option must be specified and not be empty or null."
    fi
    

    _root_path_dir=$(readlink -m "${_path}")
    if [ ! -d "${_root_path_dir}" ]
    then
        _exit_1 printf "The '--path' option specified is not a valid directory. \\nReadlink does not convert to a valid directory: 'readlink -m %s'\\n" "${_path}"
    fi
    

    # ---------------------------------------------------------------------
    # Check rclone specific info
    # ---------------------------------------------------------------------
    if command -v rclone &>/dev/null
    then
        _verb printf "Using rclone found in PATH:\\n"
        _verb printf "%s\\n" "$(command -v rclone)"
        _verb printf "%s\\n" "$(rclone --version)" 
    else
        _warn printf "rclone could not be found in PATH, so using the module: %s\\n" "/home/lmnp/knut0297/software/modulesfiles/rclone/1.57.0"
        MODULEPATH="/home/lmnp/knut0297/software/modulesfiles" module load rclone/1.57.0
        _verb printf "%s\\n" "$(command -v rclone)"
        _verb printf "%s\\n" "$(rclone --version)" 
    fi

    # Make sure the remote exists
    if ! rclone listremotes | grep -q "^$_remote:\$"
    then
        _exit_1 printf "Rclone remote does not exist: %s\\nCheck available remotes by running 'rclone listremotes', or set one up by running 'rclone init'.\\n" "$_remote"
    fi

    # Make sure access to bucket is possible
    if ! rclone lsf ${_remote}:${_bucket} &>/dev/null
    then
        _exit_1 printf "Errors occured when accessing bucket: '%s'\\nDo you have access rights to the bucket?\\nCheck the bucket access policy using 's3cmd info s3://%s'\\n" "${_bucket}" "${_bucket}"
    fi

    



    # ---------------------------------------------------------------------
    # Check to make sure this input dir does NOT already exist on ceph.
    # ---------------------------------------------------------------------


    # Count the number of ceph objects (files) listed under the input archive dir. If there are any files under this dir name already, stop.
    remote_root_path_dir_filenumber=$(rclone lsf ${_remote}:${_bucket}$(dirname ${_root_path_dir})/$(basename ${_root_path_dir}) --links --max-depth 1 2> /dev/null | wc -l)


    if [[ $remote_root_path_dir_filenumber -gt 0 ]]
    then
        _exit_1 printf "Tier 2 (ceph) already has files saved under this input 'directory name' (%s) in this bucket (%s). To prevent any possible over-writing of data on ceph, please choose a different panfs directory path to archive or a different ceph bucket.\\n" "${_root_path_dir}" "${_bucket}"
    fi



    # ---------------------------------------------------------------------
    # Create archive working dir
    # ---------------------------------------------------------------------

    _archive_date_time="$(date +"%Y-%m-%d-%H%M%S")"
    _myprefix="panfs2ceph_archive_${_archive_date_time}"
    
    # If the log_dir variable is empty or null, use the default.
    if [ -z "${_log_dir}" ]
    then
        # Default _log_dir path
        _myprefix_dir=$(dirname ${_root_path_dir})/$(basename ${_root_path_dir})___${_myprefix}

        mkdir -p ${_myprefix_dir}
        chmod --reference ${_root_path_dir} ${_myprefix_dir}
    else
        # Make sure supplied path is a real location
        _root_log_dir=$(readlink -m "${_log_dir}")
        if [ ! -d "${_root_log_dir}" ]
        then
            _exit_1 printf "The '--log_dir' option specified is not a valid directory. Does it exist? \\nReadlink does not convert to a valid directory: 'readlink -m %s'\\n" "${_log_dir}"
        else
            # Set the _log_dir as specified, after converted via readlink
            _myprefix_dir=${_root_log_dir}
        fi
    fi



    _verb printf "Archive dir name: %s\\n" ${_myprefix_dir}
    cd ${_myprefix_dir}



#     # ---------------------------------------------------------------------
#     # Check for read and write access
#     # ---------------------------------------------------------------------
# 
#     find ${_root_path_dir} -type f ! -perm -u=r >files_not_readable_by_user_${USER}.txt 2>/dev/null || true
#     find ${_root_path_dir} -type d ! -perm -u=r >directories_not_readable_by_user_${USER}.txt 2>/dev/null || true
#     find ${_root_path_dir} -type d ! -perm -u=x >directories_not_executable_by_user_${USER}.txt 2>/dev/null || true
# 
#     find ${_root_path_dir} -type f ! -perm -u=w >files_not_writable_or_deletable_by_user_${USER}.txt 2>/dev/null || true
#     find ${_root_path_dir} -type d ! -perm -u=w >directories_not_writable_or_deletable_by_user_${USER}.txt 2>/dev/null || true
#     
#     if [[ ! -s "files_not_readable_by_user_${USER}.txt" ]]
#     then
#         # Empty file, delete it.
#         rm "files_not_readable_by_user_${USER}.txt"
#     fi
#     if [[ ! -s "directories_not_readable_by_user_${USER}.txt" ]]
#     then
#         # Empty file, delete it.
#         rm "directories_not_readable_by_user_${USER}.txt"
#     fi
#     if [[ ! -s "directories_not_executable_by_user_${USER}.txt" ]]
#     then
#         # Empty file, delete it.
#         rm "directories_not_executable_by_user_${USER}.txt"
#     fi
#     if [[ ! -s "files_not_writable_or_deletable_by_user_${USER}.txt" ]]
#     then
#         # Empty file, delete it.
#         rm "files_not_writable_or_deletable_by_user_${USER}.txt"
#     fi
#     if [[ ! -s "directories_not_writable_or_deletable_by_user_${USER}.txt" ]]
#     then
#         # Empty file, delete it.
#         rm "directories_not_writable_or_deletable_by_user_${USER}.txt"
#     fi
#     
#     _permission_files=("${_root_path_dir}/*_by_user*")
#     if [[ ${#_permission_files[@]} -gt 0 ]]
#     then
#         _exit_1 printf "There are files or directories listed in the backup dir '%s' that are not readable or executable (dirs) by user: %s. These items cannot be transfered. Review them here: '%s'.\\n" "${_root_path_dir}" "${USER}" "${_myprefix_dir}"
#     fi



    # ---------------------------------------------------------------------
    # Get a file list
    # ---------------------------------------------------------------------


    if ((_delete_empty_dirs))
    then
        # "The '--delete_empty_dirs' option was specified. Any empty dirs will not be copied to ceph."
        :
    else 
        # Create a hidden file inside all empty dirs. This will ensure the empty dir will get copied as an object to ceph.
        find ${_root_path_dir} -type d -empty -print0 | xargs -I{} -0 touch {}/.empty_dir
    fi



    # Print a list of files that will be archived
    _verb printf "Creating the archive file list. This might take a while for large dirs...\\n"

    rclone lsf -R --links ${_root_path_dir} > ${_myprefix}.filelist.txt_temp1
    # _myprefix the files with full pathname
    sed -e "s|^|${_root_path_dir}/|" ${_myprefix}.filelist.txt_temp1 > ${_myprefix}.filelist.txt_temp2
    sort ${_myprefix}.filelist.txt_temp2 > ${_myprefix}.filelist.txt
    rm ${_myprefix}.filelist.txt_temp{1,2}





    # ---------------------------------------------------------------------
    # Check for filenames or pathnames that are too long
    # ---------------------------------------------------------------------

    # This check is probably pointless because I think the limit is the same for both
    # panfs and ceph -- so if files are on panfs, they should also fit on ceph.

    # On object storage (S3, ceph), filenames are irrelevant. Everything is a filename (with dirs being 
    # represented by slash delimiters). Thus, we only need to test for path length.
    # https://stackoverflow.com/questions/6870824/what-is-the-maximum-length-of-a-filename-in-s3

    pathname_max=1024

    # Are there any with filenames that are too long? They need to be less than 1024 characters (I think?).
    awk '{ PATHNAME=$0; print length(PATHNAME)"\t"PATHNAME}' ${_myprefix}.filelist.txt | awk -v pathname_max=$pathname_max -v out_file="${_myprefix}.filelist.paths_too_long.txt" '{if ($1 >= pathname_max) {print $0 > out_file}}'


    if [[ -s ${_myprefix}.filelist.paths_too_long.txt ]]
    then
        _exit_1 printf "Some pathnames are too long (> %s char) for ceph. See %s.filelist.paths_too_long.txt for a list. Shorten them before proceeding." "${pathname_max}" "${_myprefix}"
    fi


    # ---------------------------------------------------------------------
    # Determine best slurm partition
    # ---------------------------------------------------------------------

    # Capture the dns domain name for current cluster
    cluster=$(dnsdomainname | awk -F. '{print $1}')

    # Specify the appropriate slurm partition in job files
    if [[ "${cluster}" = "agate" ]]; then
        _partition="msismall"
    elif [[ "${cluster}" = "mesabi" ]]; then
        _partition="msismall"
    elif [[ "${cluster}" = "mangi" ]]; then
        _partition="msismall"
    else
        _partition="msismall"
    fi


    #######################################################################
    # Copy
    #######################################################################


    tee ${_myprefix}.1_copy.slurm << HEREDOC > /dev/null
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=${_threads}
#SBATCH --time=24:00:00
#SBATCH --mem=32gb
#SBATCH --error=%x.e%j
#SBATCH --output=%x.o%j
#SBATCH --mail-type=BEGIN,END,FAIL
#SBATCH --mail-user=$USER@umn.edu
#SBATCH --partition=$_partition

echo "[panfs2ceph "\$(date)"] Script start."


# ---------------------------------------------------------------------
# Variables
# ---------------------------------------------------------------------


_root_path_dir=$_root_path_dir
_myprefix=${_myprefix}
_myprefix_dir=${_myprefix_dir}
_dry_run=${_dry_run}
_remote=${_remote}
_bucket=${_bucket}
_threads=${_threads}



# ---------------------------------------------------------------------
# Load software
# ---------------------------------------------------------------------

cd \${_myprefix_dir}

MODULEPATH="/home/lmnp/knut0297/software/modulesfiles" module load rclone/1.57.0




# ---------------------------------------------------------------------
# rclone
# ---------------------------------------------------------------------



# Copy the files to ceph
rclone copy "/" "\${_remote}:\${_bucket}" --files-from-raw "\${_myprefix}.filelist.txt" --links \${_dry_run} -v --transfers \${_threads} --s3-chunk-size 50M --s3-upload-concurrency 10

if [ ! \$? -eq 0 ]
then
    # rclone exit status was not zero
    echo "[panfs2ceph "\$(date)"] Rclone copy process: FAIL" 2>&1 | tee \${_myprefix}.COPY_FAILED
else
    echo "[panfs2ceph "\$(date)"] Rclone copy process: SUCCESS"
fi




# ---------------------------------------------------------------------
# Verify
# ---------------------------------------------------------------------


# Sleep for a short time to allow rclone to properly list files. If done without
# a short break, rclone lsf seems fail.
echo "[panfs2ceph "\$(date)"] Starting verification..."
#echo "[panfs2ceph "\$(date)"] Waiting for 30 sec to allow ceph to index files..."
#sleep 30
echo "[panfs2ceph "\$(date)"] Getting file list from ceph..."

# Nudge rclone to list the files on bucket (refresh). This seems to help avoid random errors.
#rclone lsf -R --links \${_remote}: --max-depth 1 > /dev/null
#rclone lsf -R --links \${_remote}:\${_bucket} --max-depth 1 > /dev/null


# Get a file list from ceph (after the transfer)
rclone lsf -R --links \${_remote}:\${_bucket}\${_root_path_dir} > \${_myprefix}.filelist_from_ceph.txt_temp1

# _myprefix the files with full pathname
sed -e "s|^|\${_root_path_dir}/|" \${_myprefix}.filelist_from_ceph.txt_temp1 > \${_myprefix}.filelist_from_ceph.txt_temp2
sort \${_myprefix}.filelist_from_ceph.txt_temp2 > \${_myprefix}.filelist_from_ceph.txt
rm \${_myprefix}.filelist_from_ceph.txt_temp{1,2}



# Compare the sorted files on ceph (just uploaded) against the list of files from panfs
comm -23 <(sort \${_myprefix}.filelist.txt) <(sort \${_myprefix}.filelist_from_ceph.txt) > \${_myprefix}.files_on_panfs_but_not_ceph.txt

comm -13 <(sort \${_myprefix}.filelist.txt) <(sort \${_myprefix}.filelist_from_ceph.txt) > \${_myprefix}.files_on_ceph_but_not_panfs.txt


if [[ ! -s \${_myprefix}.files_on_panfs_but_not_ceph.txt ]]
then
   # The file above is empty.
   echo "[panfs2ceph "\$(date)"] There are no files in '\${_myprefix}.filelist.txt' that are missing from ceph."
else
    echo "[panfs2ceph "\$(date)"] There are files listed in '\${_myprefix}.filelist.txt' that are not found on ceph."
    echo "[panfs2ceph "\$(date)"] Review: '\${_myprefix}.files_on_panfs_but_not_ceph.txt'" >&2
fi


if [[ ! -s \${_myprefix}.files_on_ceph_but_not_panfs.txt ]]
then
   # The file above is empty.
   echo "[panfs2ceph "\$(date)"] There are no files on ceph that are not listed in '\${_myprefix}.filelist.txt'."
else
    echo "[panfs2ceph "\$(date)"] There are files on ceph that are not listed in '\${_myprefix}.filelist.txt'." 
    echo "[panfs2ceph "\$(date)"] Review: '\${_myprefix}.files_on_ceph_but_not_panfs.txt'" >&2
fi







# ---------------------------------------------------------------------
# Job summary info
# ---------------------------------------------------------------------

echo "[panfs2ceph "\$(date)"] Script end."

if [ ! -z \${SLURM_JOB_ID+x} ]; then
    scontrol show job "\${SLURM_JOB_ID}"
    sstat -j "\${SLURM_JOB_ID}" --format=JobID,MaxRSS,MaxVMSize,NTasks,MaxDiskWrite,MaxDiskRead
fi



HEREDOC







    #######################################################################
    # Delete
    #######################################################################


    tee ${_myprefix}.2_delete.slurm << HEREDOC > /dev/null
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --time=24:00:00
#SBATCH --mem=32gb
#SBATCH --error=%x.e%j
#SBATCH --output=%x.o%j
#SBATCH --mail-type=BEGIN,END,FAIL
#SBATCH --mail-user=$USER@umn.edu
#SBATCH --partition=$_partition

echo "[panfs2ceph "\$(date)"] Script start."


# ---------------------------------------------------------------------
# Variables
# ---------------------------------------------------------------------


_root_path_dir=${_root_path_dir}
_myprefix=${_myprefix}
_myprefix_dir=${_myprefix_dir}
_dry_run=${_dry_run}




# ---------------------------------------------------------------------
# Software
# ---------------------------------------------------------------------

cd \${_myprefix_dir}

MODULEPATH="/home/lmnp/knut0297/software/modulesfiles" module load rsync/3.1.2

# ---------------------------------------------------------------------
# Delete files from panfs
# ---------------------------------------------------------------------

# Delete all files in the file list
# Use rsync to delte files because it's faster than "rm"

_empty_dir=\${_myprefix_dir}/empty_dir

mkdir -p \${_empty_dir}

rsync \${_dry_run} -v -r --force --delete \${_empty_dir}/ \${_root_path_dir}/



if [ ! \$? -eq 0 ]
then
    # rclone exit status was not zero
    echo "[panfs2ceph "\$(date)"] Delete process: FAIL" 2>&1 | tee \${_myprefix}.DELETE_FAILED
else
    echo "[panfs2ceph "\$(date)"] Delete process: SUCCESS"
fi

# Delete the original archive dir and the temp empty dir
rm -rf \${_root_path_dir}
rm -rf \${_empty_dir}





# ---------------------------------------------------------------------
# Job summary info
# ---------------------------------------------------------------------

echo "[panfs2ceph "\$(date)"] Script end."

if [ ! -z \${SLURM_JOB_ID+x} ]; then
    scontrol show job "\${SLURM_JOB_ID}"
    sstat -j "\${SLURM_JOB_ID}" --format=JobID,MaxRSS,MaxVMSize,NTasks,MaxDiskWrite,MaxDiskRead
fi



HEREDOC







    #######################################################################
    # Restore
    #######################################################################


    tee ${_myprefix}.3_restore.slurm << HEREDOC > /dev/null
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=${_threads}
#SBATCH --time=24:00:00
#SBATCH --mem=32gb
#SBATCH --error=%x.e%j
#SBATCH --output=%x.o%j
#SBATCH --mail-type=BEGIN,END,FAIL
#SBATCH --mail-user=$USER@umn.edu
#SBATCH --partition=$_partition

echo "[panfs2ceph "\$(date)"] Script start."


# ---------------------------------------------------------------------
# Variables
# ---------------------------------------------------------------------


_root_path_dir=${_root_path_dir}
_myprefix=${_myprefix}
_myprefix_dir=${_myprefix_dir}
_dry_run=${_dry_run}
_remote=${_remote}
_bucket=${_bucket}
_threads=${_threads}



# ---------------------------------------------------------------------
# Load software
# ---------------------------------------------------------------------



MODULEPATH="/home/lmnp/knut0297/software/modulesfiles" module load rclone/1.57.0



# ---------------------------------------------------------------------
# Check if original dir still exists on panfs
# ---------------------------------------------------------------------

if [ -d "\${_root_path_dir}" ]
then
    echo "[panfs2ceph "\$(date)"] The original dir (\${_root_path_dir}) still exists on panfs. Stopping restore process. Change the dir name on panfs and re-run restore script." >&2
    exit 1
fi



# ---------------------------------------------------------------------
# rclone
# ---------------------------------------------------------------------

# Nudge rclone to list the files on bucket (refresh). This seems to help avoid random errors.
rclone lsf -R --links \${_remote}:\${_bucket} --max-depth 1 > /dev/null


# Copy the files from ceph back to panfs
rclone copy "\${_remote}:\${_bucket}\${_root_path_dir}" "\${_root_path_dir}" --links \${_dry_run} -v --transfers \${_threads} --s3-chunk-size 50M --s3-upload-concurrency 10


if [ ! \$? -eq 0 ]
then
    # rclone exit status was not zero
    echo "[panfs2ceph "\$(date)"] rclone restore process: FAIL" 2>&1 | tee \${_myprefix}.RESTORE_FAILED
else
    echo "[panfs2ceph "\$(date)"] rclone restore process: SUCCESS"
fi



# ---------------------------------------------------------------------
# Job summary info
# ---------------------------------------------------------------------

echo "[panfs2ceph "\$(date)"] Script end."

if [ ! -z \${SLURM_JOB_ID+x} ]; then
    scontrol show job "\${SLURM_JOB_ID}"
    sstat -j "\${SLURM_JOB_ID}" --format=JobID,MaxRSS,MaxVMSize,NTasks,MaxDiskWrite,MaxDiskRead
fi







HEREDOC






    #######################################################################
    # Readme
    #######################################################################



    tee ${_myprefix}.readme.md << HEREDOC > /dev/null
# panfs2ceph archive

Archive initated (Y-m-d-HMS):  
${_archive_date_time}   

## Introduction

This directory (\`${_root_path_dir}\`) was archived from MSI's *panfs* (tier 1) storage to its *ceph* tier 2 storge. All data inside the directory was copied using \`rclone\` which uses MD5 checksums on every transfer to ensure data integrity. After the data was copied, it was deleted from \`panfs\`. 


## How this works

* A program called \`cephtools panfs2ceph\` found all the files in the directory above and created a file list text file.
* Then the program created a slurm job file (bash script) that can be run to copy all files from panfs to ceph.
* The program also created a slurm job file (bash script) to delete the original data from panfs.
* The program also created this README file.
* The program does not actually run these slrum job scripts. You must do that manually after reviewing the file list, etc.
* After running the slurm job scripts, you should review the stderr and stdout log files and the new file lists that get created during the \`*.1_copy.slurm\` job.


## Notes:

* A new dir is created next to the original that contains these archive-related files and scripts. This dir should have the same permissions as the original dir (that got archived). 
* File modification times/dates should be preserved between transfers. However, if files are transferred from ceph back to panfs, the original modification times of directories will be lost (i.e. block storage like tier 2 does not have directories, so their mod times can't be preserved).



## Variables


Variable Name | Value 
-----------|----------
Archive date and time | ${_archive_date_time}
Directory to archive | ${_root_path_dir}
Archive transfer prefix | ${_myprefix}
Directory path for archive files | ${_myprefix_dir}
Ceph UserID | $(s3info info | grep username | awk -F': ' '{print $2}')
Ceph Remote Name | ${_remote}
Ceph Bucket Name | ${_bucket}
User doing transfer | $USER


## How to access the original data

### Browse the data on ceph

You can use a variety of tools to browse data stored on ceph. For example:

1. rclone (e.g. \`rclone ls ${_remote}:${_bucket}${_root_path_dir}\`). This requires a properly configured \`~/.config/rclone/rclone.conf\` file. 
2. s3cmd (e.g. \`s3cmd ls -r s3://${_bucket}${_root_path_dir}\`). Note: different MSI users might use different rclone remote names -- use *your* remote name. 
3. GUI based file browser (e.g. FileZilla, Panic's Transmit, CyberDuck, etc.)


### Browse the data via panfs \`snapshot\`

The original data was deleted, but might still be stored on \`panfs\` as a snapshot for a short period of time (approximately 1 month or less). If this data was archived to ceph and deleted from panfs, it might be recovered from panfs for a short time. Review the available snapshots here:

    ${_root_path_dir}/.snapshot


### Restore from ceph back to panfs

Depending on user permissions, the data can be copied from ceph back to panfs using \`rclone\` or other methods. For example, running \`${_myprefix}.3_restore.slurm\` job script should copy the data from ceph back to panfs in its original location.



HEREDOC




    #######################################################################
    # Print instructions to terminal
    #######################################################################


    #read -r -d '' instructions_message << HEREDOC > /dev/null
    # The unix "read" command does not return a zero exit code. Use a temp function instead.
    # https://stackoverflow.com/a/8088167/2367748
    heredoc2var(){ IFS='\n' read -r -d '' ${1} || true; }
    
    heredoc2var instructions_message << HEREDOC > /dev/null

---------------------------------------------------------------------
cephtools panfs2ceph summary

Options used:
dry_run=${_dry_run}
delete_empty_dirs=${_delete_empty_dirs}
remote=${_remote}
bucket=${_bucket}
threads=${_threads}

Archive dir: 
${_root_path_dir}

Archive dir transfer scripts:
${_myprefix_dir}

Achrive transfer files created -- but you're not done yet!
Next steps:
1. Move into transfer dir: cd ${_myprefix_dir}
2. Review the *.readme.md file for details.
3. Review the *.filelist.txt file. The files in this list will be copied to ceph and delted from panfs.
4. Launch the copy jobfile: sbatch ${_myprefix}.1_copy.slurm
5. After sucessful copy, launch the delete jobfile: sbatch ${_myprefix}.2_delete.slurm
6. After the data has been deleted from panfs -- and you need it back in the same location, launch the restore jobfile: sbatch ${_myprefix}.3_restore.slurm


VERSION: ${VERSION_SHORT}
QUESTIONS: Todd Knutson (knut0297@umn.edu)
REPO: https://github.umn.edu/knut0297org/cephtools
---------------------------------------------------------------------
HEREDOC


    echo "$instructions_message"


}



