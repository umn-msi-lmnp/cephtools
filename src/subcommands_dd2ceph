


# ---------------------------------------------------------------------
# dd2ceph
# ---------------------------------------------------------------------

describe "dd2ceph" <<HEREDOC
---------------------------------------------------------------------
Usage:
    ${_ME} dd2ceph [options] --bucket <BUCKET> --remote <REMOTE> --path <DIR_PATH>

Options:                            
    -b|--bucket <STRING>    Name of the ceph bucket that data should be used for the 
                            transfer. [Default = "$(id -ng)-data-archive"]
    
    -p|--path <STRING>      Absolute or relative path to the directory that should be 
                            transfered. [Default = "/home/$(id -ng)/data_delivery"]
                            
    -l|--log_dir <STRING>   Absolute or relative path to the directory where log files 
                            are saved. [Default = "/home/$(id -ng)/shared/dd2ceph"]
    
    -d|--dry_run            Dry run option will be applied to rclone commands. Nothing 
                            transfered or deleted when scripts run.
    
    -v|--verbose            Verbose mode (print additional info).
                            
    -t|--threads <INT>      Threads to use for uploading with rclone. [Default = 16].
    

Description:
  Archiving tool to copy any new data from the "data_delivery" directory to tier 2 (ceph).
  
Help (print this screen):
    ${_ME} help dd2ceph

Version: ${VERSION_SHORT}
Questions: Todd Knutson (knut0297@umn.edu) or Christine O'Connor (oconnorc@umn.edu)
GitHub: https://github.umn.edu/lmnp/cephtools
---------------------------------------------------------------------
HEREDOC

dd2ceph() {
#     echo ALL ARGS:
#     echo "${@:-}"
#     echo "${#}"


    
    # Parse Options ###############################################################

    # Initialize program option variables.
    local _bucket="$(id -ng)-data-archive"
    local _path="/home/$(id -ng)/data_delivery"
    local _log_dir="/home/$(id -ng)/shared/dd2ceph"
    local _dry_run=
    local _verbose=
    local _threads="16"

    # __get_option_value()
    #
    # Usage:
    #   __get_option_value <option> <value>
    #
    # Description:
    #  Given a flag (e.g., -e | --example) return the value or exit 1 if value
    #  is blank or appears to be another option.
    __get_option_value() {
      local __arg="${1:-}"
      local __val="${2:-}"
      
      if [[ -n "${__val:-}" ]] && [[ ! "${__val:-}" =~ ^- ]]
      then
        printf "%s\\n" "${__val}"
      else
        _exit_1 printf "%s requires a valid argument.\\n" "${__arg}"
      fi
    }


    # For flags (i.e. no corresponding value), do not shift inside the case testing
    # statement. For options with required value, shift inside case testing statement, 
    # so the loop moves twice. 
    while ((${#}))
    do
        __arg="${1:-}"
        __val="${2:-}"

        case "${__arg}" in
        -d|--dry_run)
            _dry_run="--dry-run"
            ;;
        -v|--verbose)
            _verbose=1
            ;;
        -b|--bucket)
            _bucket="$(__get_option_value "${__arg}" "${__val:-}")"
            shift
            ;;
        -p|--path)
            _path="$(__get_option_value "${__arg}" "${__val:-}")"
            shift
            ;;
        -l|--log_dir)
            _log_dir="$(__get_option_value "${__arg}" "${__val:-}")"
            shift
            ;;
        -t|--threads)
            _threads="$(__get_option_value "${__arg}" "${__val:-}")"
            shift
            ;;
        --endopts)
            # Terminate option parsing.
            break
            ;;
        -*)
            _exit_1 printf "Unexpected option: %s\\n" "${__arg}"
            ;;
        *)
            describe --get dd2ceph
            _exit_1 printf "Unexpected positional arg: %s\\n" "${__arg}"
            ;;
        esac

        shift
    done


    # ---------------------------------------------------------------------
    # Check and print input options
    # ---------------------------------------------------------------------

    _verb printf -- "Program options used:\\n"
    _verb printf -- "--bucket: %s\\n" "$_bucket"
    _verb printf -- "--path: %s\\n" "$_path"
    _verb printf -- "--log_dir: %s\\n" "$_log_dir"
    _verb printf -- "--dry_run: %s\\n" "$_dry_run"
    _verb printf -- "--verbose: %s\\n" "$_verbose"
    _verb printf -- "--threads: %s\\n" "$_threads"


    
    # If required options are empty or null, exit.
    #if [ -z "${_remote}" ]
    #then
    #    _exit_1 printf "The '--remote' option must be specified and not be empty or null."
    #fi
    

    _root_path_dir=$(readlink -m "${_path}")
    if [ ! -d "${_root_path_dir}" ]
    then
        _exit_1 printf "The '--path' option specified is not a valid directory. \\nReadlink does not convert to a valid directory: 'readlink -m %s'\\n" "${_path}"
    fi


    if command -v rclone &>/dev/null
    then
        RCLONE_MINOR_VER="$(rclone --version | head -n 1 | sed 's/rclone v..//' | sed 's/\..*$//')"
        if [ "$RCLONE_MINOR_VER" -ge "57" ]
        then
            _verb printf "Using rclone found in PATH:\\n"
            _verb printf "%s\\n" "$(command -v rclone)"
            _verb printf "%s\\n" "$(rclone --version)"
        else
            _warn printf "rclone in your PATH was a version less than 1.57, so using the module: %s\\n" "/home/lmnp/knut0297/software/modulesfiles/rclone/1.57.0"
            MODULEPATH="/home/lmnp/knut0297/software/modulesfiles" module load rclone/1.57.0
            _verb printf "%s\\n" "$(command -v rclone)"
            _verb printf "%s\\n" "$(rclone --version)" 
        fi
    else
        _warn printf "rclone could not be found in PATH, so using the module: %s\\n" "/home/lmnp/knut0297/software/modulesfiles/rclone/1.57.0"
        MODULEPATH="/home/lmnp/knut0297/software/modulesfiles" module load rclone/1.57.0
        _verb printf "%s\\n" "$(command -v rclone)"
        _verb printf "%s\\n" "$(rclone --version)" 
    fi


    if [ ! -d "${_log_dir}" ]
    then
        _warn printf "The '--log_dir' option specified is not a valid directory. Creating the dir with g+rwx permissions: '%s'\\n" "${_log_dir}"
        mkdir -p ${_log_dir}
        chmod g+rwx ${_log_dir}
    fi

    # Make sure the remote exists
#    if ! rclone listremotes | grep -q "^$_remote:\$"
#    then
#        _exit_1 printf "Rclone remote does not exist: %s\\nCheck available remotes by running 'rclone listremotes', or set one up by running 'rclone init'.\\n" "$_remote"
#    fi

     # ---------------------------------------------------------------------
     # Set the environment
     # ---------------------------------------------------------------------
     # set the environment here as well so I can check bucket access
     export RCLONE_CONFIG_MYREMOTE_TYPE=s3


     export RCLONE_CONFIG_MYREMOTE_ENV_AUTH=FALSE
     export RCLONE_CONFIG_MYREMOTE_ACCESS_KEY_ID=$(s3info --keys | awk '{print $1}')
     export RCLONE_CONFIG_MYREMOTE_SECRET_ACCESS_KEY=$(s3info --keys | awk '{print $2}')
     export RCLONE_CONFIG_MYREMOTE_ENDPOINT=s3.msi.umn.edu
     export RCLONE_CONFIG_MYREMOTE_ACL=private
     export RCLONE_CONFIG_MYREMOTE_PROVIDER=Ceph

       #rclone lsd MYREMOTE:

    # Make sure access to bucket is possible
    if ! rclone lsf MYREMOTE:${_bucket} &>/dev/null
    then
        _exit_1 printf "Errors occured when accessing bucket: '%s'\\nDoes the bucket exist?\\nDo you have access rights to the bucket?\\nCheck the bucket access policy using 's3cmd info s3://%s'\\nOr the MSI group PI should create the bucket using 's3cmd mb s3://%s'\\n" "${_bucket}" "${_bucket}" "${_bucket}"
    fi

    # ---------------------------------------------------------------------
    # Check s3cmd specific info
    # ---------------------------------------------------------------------

    # Only need to check that we can access s3cmd commands
    S3CMD="$(which s3cmd)"
    if command -v s3cmd &> /dev/null
    then
        _verb printf "Using s3cmd found in PATH: %s\\n" "$(which s3cmd)"
        _verb printf "%s\\n" "$(s3cmd --version)" 
    else
        _exit_1 printf "s3cmd could not be found in PATH\\n"
    fi

    # check that bucket exists
      # Make sure access to bucket is possible
    if ! s3cmd ls s3://${_bucket} &>/dev/null; then
        _info printf "Bucket was made: %s" "${_bucket}"
    else
        _exit_1 printf "Errors occured when accessing bucket: '%s'\\nDo you have access rights to the bucket?\\nCheck the bucket access policy using 's3cmd info s3://%s'\\n If bucket does not exist run cephtools bucketpolicy first\\n" "${_bucket}" "${_bucket}"
    fi

    


    # ---------------------------------------------------------------------
    # Create archive working dir
    # ---------------------------------------------------------------------

    _archive_date_time="$(date +"%Y-%m-%d-%H%M%S")"
    _myprefix="dd2ceph_${_archive_date_time}"
    _myprefix_dir=${_log_dir}/${_bucket}___${_myprefix}

    _verb printf "Archive dir name: %s\\n" ${_myprefix_dir}
    mkdir -p ${_myprefix_dir}
    chmod g+rwx ${_myprefix_dir}
    cd ${_myprefix_dir}






    # ---------------------------------------------------------------------
    # Get a file list
    # ---------------------------------------------------------------------



    # Print a list of files that will be archived
    _verb printf "Creating the archive file list. This might take a while for large dirs...\\n"

    rclone lsf -R --copy-links ${_root_path_dir} > ${_myprefix}.filelist.txt
    # _myprefix the files with full pathname
    sed -i -e "s|^|${_root_path_dir}/|" ${_myprefix}.filelist.txt





    # ---------------------------------------------------------------------
    # Check for filenames or pathnames that are too long
    # ---------------------------------------------------------------------

    # This check is probably pointless because I think the limit is the same for both
    # panfs and ceph -- so if files are on panfs, they should also fit on ceph.

    # On object storage (S3, ceph), filenames are irrelevant. Everything is a filename (with dirs being 
    # represented by slash delimiters). Thus, we only need to test for path length.
    # https://stackoverflow.com/questions/6870824/what-is-the-maximum-length-of-a-filename-in-s3

    pathname_max=1024

    # Are there any with filenames that are too long? They need to be less than 1024 characters (I think?).
    awk '{ PATHNAME=$0; print length(PATHNAME)"\t"PATHNAME}' ${_myprefix}.filelist.txt | awk -v pathname_max=$pathname_max -v out_file="${_myprefix}.filelist.paths_too_long.txt" '{if ($1 >= pathname_max) {print $0 > out_file}}'


    if [[ -s ${_myprefix}.filelist.paths_too_long.txt ]]
    then
        _exit_1 printf "Some pathnames are too long (> %s char) for ceph. See %s.filelist.paths_too_long.txt for a list. Shorten them before proceeding." "${pathname_max}" "${_myprefix}"
    fi



    # ---------------------------------------------------------------------
    # Determine best slurm partition
    # ---------------------------------------------------------------------

    # Capture the dns domain name for current cluster
    cluster=$(dnsdomainname | awk -F. '{print $1}')

    # Specify the appropriate slurm partition in job files
    if [[ "${cluster}" = "agate" ]]; then
        _partition="msismall"
    elif [[ "${cluster}" = "mesabi" ]]; then
        _partition="msismall"
    elif [[ "${cluster}" = "mangi" ]]; then
        _partition="msismall"
    else
        _partition="msismall"
    fi




    #######################################################################
    # Copy
    #######################################################################


    tee ${_myprefix}.1_copy.slurm << EOF > /dev/null
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=${_threads}
#SBATCH --time=24:00:00
#SBATCH --mem=32gb
#SBATCH --error=%x.e%j
#SBATCH --output=%x.o%j
#SBATCH --mail-type=BEGIN,END,FAIL
#SBATCH --mail-user=$USER@umn.edu
#SBATCH --partition=$_partition

echo "[dd2ceph "\$(date)"] Script start."


# ---------------------------------------------------------------------
# Variables
# ---------------------------------------------------------------------


_root_path_dir=$_root_path_dir
_myprefix=${_myprefix}
_myprefix_dir=${_myprefix_dir}
_dry_run=${_dry_run}
#_remote=${_remote}
_bucket=${_bucket}
_threads=${_threads}



# ---------------------------------------------------------------------
# Load software
# ---------------------------------------------------------------------

cd \${_myprefix_dir}

MODULEPATH="/home/lmnp/knut0297/software/modulesfiles" module load rclone/1.57.0


# ---------------------------------------------------------------------
# Set the environment
# ---------------------------------------------------------------------

export RCLONE_CONFIG_MYREMOTE_TYPE=s3


export RCLONE_CONFIG_MYREMOTE_ENV_AUTH=FALSE
export RCLONE_CONFIG_MYREMOTE_ACCESS_KEY_ID=$(s3info --keys | awk '{print $1}')
export RCLONE_CONFIG_MYREMOTE_SECRET_ACCESS_KEY=$(s3info --keys | awk '{print $2}')
export RCLONE_CONFIG_MYREMOTE_ENDPOINT=s3.msi.umn.edu
export RCLONE_CONFIG_MYREMOTE_ACL=private
export RCLONE_CONFIG_MYREMOTE_PROVIDER=Ceph

rclone lsd MYREMOTE:


# ---------------------------------------------------------------------
# rclone
# ---------------------------------------------------------------------



# Copy the files to ceph
#rclone copy "/" "\${_remote}:\${_bucket}" --files-from-raw "\${_myprefix}.filelist.txt" --copy-links \${_dry_run} -v --transfers \${_threads} --s3-chunk-size 50M --s3-upload-concurrency 10
# new version
rclone copy "/" "MYREMOTE:\${_bucket}" --files-from-raw "\${_myprefix}.filelist.txt" --copy-links \${_dry_run} -v --transfers \${_threads} --s3-chunk-size 50M --s3-upload-concurrency 10


if [ ! \$? -eq 0 ]
then
    # rclone exit status was not zero
    echo "[dd2ceph "\$(date)"] Rclone copy process: FAIL" 2>&1 | tee \${_myprefix}.COPY_FAILED
else
    echo "[dd2ceph "\$(date)"] Rclone copy process: SUCCESS"
fi




# ---------------------------------------------------------------------
# Verify
# ---------------------------------------------------------------------



echo "[dd2ceph "\$(date)"] Starting verification..."
echo "[dd2ceph "\$(date)"] Getting file list from ceph..."
rclone lsf -R --copy-links MYREMOTE:\${_bucket}\${_root_path_dir} > \${_myprefix}.filelist_from_ceph.txt

# _myprefix the files with full pathname
sed -i -e "s|^|\${_root_path_dir}/|" \${_myprefix}.filelist_from_ceph.txt




# Compare the sorted files on ceph (just uploaded) against the list of files from panfs
comm -23 <(sort \${_myprefix}.filelist.txt) <(sort \${_myprefix}.filelist_from_ceph.txt) > \${_myprefix}.files_on_panfs_but_not_ceph.txt

comm -13 <(sort \${_myprefix}.filelist.txt) <(sort \${_myprefix}.filelist_from_ceph.txt) > \${_myprefix}.files_on_ceph_but_not_panfs.txt


if [[ ! -s \${_myprefix}.files_on_panfs_but_not_ceph.txt ]]
then
   # The file above is empty.
   echo "[dd2ceph "\$(date)"] There are no files in '\${_myprefix}.filelist.txt' that are missing from ceph."
else
    echo "[dd2ceph "\$(date)"] There are files listed in '\${_myprefix}.filelist.txt' that are not found on ceph."
    echo "[dd2ceph "\$(date)"] Review: '\${_myprefix}.files_on_panfs_but_not_ceph.txt'" >&2
fi


if [[ ! -s \${_myprefix}.files_on_ceph_but_not_panfs.txt ]]
then
   # The file above is empty.
   echo "[dd2ceph "\$(date)"] There are no files on ceph that are not listed in '\${_myprefix}.filelist.txt'."
else
    echo "[dd2ceph "\$(date)"] There are files on ceph that are not listed in '\${_myprefix}.filelist.txt'." 
    echo "[dd2ceph "\$(date)"] Review: '\${_myprefix}.files_on_ceph_but_not_panfs.txt'" >&2
fi





# ---------------------------------------------------------------------
# Job summary info
# ---------------------------------------------------------------------

echo "[dd2ceph "\$(date)"] Script end."

if [ ! -z \${SLURM_JOB_ID+x} ]; then
    scontrol show job "\${SLURM_JOB_ID}"
    sstat -j "\${SLURM_JOB_ID}" --format=JobID,MaxRSS,MaxVMSize,NTasks,MaxDiskWrite,MaxDiskRead
fi







EOF


# ---------------------------------------------------------------------
# Launch jobs
# ---------------------------------------------------------------------


# no dry run option here, since dry_run means that rclone doesn't actually transfer anything 

sbatch_output=$(sbatch "${_myprefix}.1_copy.slurm")
job_id=$(echo "${sbatch_output}" | awk '{print $4}')
sbatch_job_ids+=("${job_id}")
_verb printf "%s\n" "${sbatch_output}"
_info printf "\${_myprefix}.1_copy.slurm job file created and launched.\n"





    #######################################################################
    # Readme
    #######################################################################



    tee ${_myprefix}.readme.md << EOF > /dev/null


# dd2ceph archive

Archive initated (Y-m-d-HMS):  
${_archive_date_time}   

## Introduction

This directory (\`${_root_path_dir}\`) was archived from MSI's \`panfs\` (tier 1) storage to its \`ceph\` tier 2 storge. All new data inside the directory was copied using \`rclone\` which uses MD5 checksums on every transfer to ensure data integrity. Only new files were transfered (based on file size and date) from panfs to ceph. 


## How this works

* A program called \`dd2ceph\` found all the files in the directory above (old and new) and created a file list text file.
* Only new files are transfered.
* Then the program created and ran a slurm job file (bash script) to copy all files from panfs to ceph.
* The program also created this README file.


## Notes:

* A new dir is created that contains these archive-related files and scripts.
* File modification times/dates should be preserved between transfers. However, if files are transferred from ceph back to panfs, the original modification times of directories will be lost (i.e. block storage like tier 2 does not have directories, so their mod times can't be preserved).



## Variables


Variable Name | Value 
-----------|----------
Archive date and time | ${_archive_date_time}
Directory to archive | ${_root_path_dir}
Directory name for archive files | ${_myprefix}
Directory path for archive files | ${_myprefix_dir}
Ceph UserID | $(s3info info | grep username | awk -F': ' '{print $2}')
Ceph Bucket Name | ${_bucket}
User doing transfer | $USER


## How to access the original data

### Browse the data on ceph

You can use a variety of tools to browse data stored on ceph. For example:

1. rclone (e.g. \`rclone ls MYREMOTE:${_bucket}${_root_path_dir}\`). This requires a properly configured \`~/.config/rclone/rclone.conf\` file. 
2. s3cmd (e.g. \`s3cmd ls -r s3://${_bucket}${_root_path_dir}\`).  
3. GUI based file browser (e.g. FileZilla, Panic's Transmit, CyberDuck, etc.)



### Restore from ceph back to panfs

The data can be copied from ceph back to panfs using \`rclone\` or other methods. Ideally, this ceph bucket (${_bucket}) should remain the main storage location for the data_delivery file. However, if you need to process some of these files, we suggest you copy the files from ceph to somewhere on MSI global scratch "/global.scratch/", so the files do not take up storage space in the home dir. 




EOF


    # ---------------------------------------------------------------------
    # Allow permissions for group
    # ---------------------------------------------------------------------

    find "${_myprefix_dir}" -type d -print0 | xargs -0 chmod ug+rwxs,o-rwx
    find "${_myprefix_dir}" -type f -print0 | xargs -0 chmod ug+rw,o-rwx


    #######################################################################
    # Print instructions to terminal
    #######################################################################



    #read -r -d '' instructions_message << EOF > /dev/null
    # The unix "read" command does not return a zero exit code. Use a temp function instead.
    # https://stackoverflow.com/a/8088167/2367748
    heredoc2var(){ IFS='\n' read -r -d '' ${1} || true; }
    
    heredoc2var instructions_message << HEREDOC > /dev/null

---------------------------------------------------------------------
cephtools dd2ceph summary

Achrive transfer files created and file transfer script started! 
Next steps:
1. Move into log dir: cd ${_myprefix_dir}
2. Review the *.readme.md file for details.
3. Review the *.filelist.txt file. Any files not already on ceph, will be copied to ceph.


VERSION: ${VERSION_SHORT}
QUESTIONS: Todd Knutson (knut0297@umn.edu)
REPO: https://github.umn.edu/knut0297org/cephtools
---------------------------------------------------------------------
HEREDOC


    echo "$instructions_message"


}



